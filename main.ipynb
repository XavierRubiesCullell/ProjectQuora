{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "# Files\n",
    "import input_net\n",
    "import utils\n",
    "\n",
    "# Packages\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Transformers\n",
    "from transformers import AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer\n",
    "from transformers import BartConfig, BartForSequenceClassification, BartTokenizer\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer\n",
    "from transformers import CamembertConfig, CamembertForSequenceClassification, CamembertTokenizer\n",
    "from transformers import DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from transformers import FlaubertConfig, FlaubertForSequenceClassification, FlaubertTokenizer\n",
    "from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer\n",
    "from transformers import XLMConfig, XLMForSequenceClassification, XLMTokenizer\n",
    "from transformers import XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "from transformers import XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                       Model                                   Tokenizer               Pretrained weights shortcut \n",
    "MODEL_CLASSES = {\n",
    "    'albert': (         AlbertForSequenceClassification,        AlbertTokenizer,        'albert-large-v2'),\n",
    "    'bart': (           BartForSequenceClassification,          BartTokenizer,          'bart-large'),\n",
    "    'bert': (           BertForSequenceClassification,          BertTokenizer,          'bert-base-uncased'),\n",
    "    'camembert': (      CamembertForSequenceClassification,     CamembertTokenizer,     'camembert-base'),\n",
    "    'distilbert': (     DistilBertForSequenceClassification,    DistilBertTokenizer,    'distilbert-base-uncased'),\n",
    "    'flaubert': (       FlaubertForSequenceClassification,      FlaubertTokenizer,      'flaubert-base-uncased'),\n",
    "    'roberta': (        RobertaForSequenceClassification,       RobertaTokenizer,       'roberta-base'),\n",
    "    'xlm': (            XLMForSequenceClassification,           XLMTokenizer,           'xlm-mlm-en-2048'),\n",
    "    'xlm_roberta':(     XLMRobertaForSequenceClassification,    XLMRobertaTokenizer,    'xlm-roberta-base'),\n",
    "    'xlnet': (          XLNetForSequenceClassification,         XLNetTokenizer,         'xlnet-base-cased')\n",
    "}\n",
    "\n",
    "args = {\n",
    "    'model_type': 'bert',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'max_seq_length': 70,\n",
    "    'batch_size': 16, \n",
    "    'epochs': 1,\n",
    "    'learning_rate': 4e-5,\n",
    "    'num_training_steps': 1000,\n",
    "    'num_warmup_steps': 100,\n",
    "    'max_grad_norm': 1.0\n",
    "}\n",
    "\n",
    "model_class, tokenizer_class, pretrained_model = MODEL_CLASSES[args['model_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = \"data/train.csv\"\n",
    "TEST = \"data/test.csv\"\n",
    "INPUT_NET = 'data/input' + str(args['max_seq_length']) + '_' + pretrained_model + '.csv'\n",
    "\n",
    "if not path.exists(INPUT_NET):\n",
    "    df = input_net.create_input(TRAIN, INPUT_NET, tokenizer_class, pretrained_model, args)\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_NET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Functions -> quan funcioni tot cridar-ho desde net.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, target, batch_size):\n",
    "    data = np.array(data)\n",
    "    target = np.array(target)\n",
    "    nsamples = len(data)\n",
    "    perm = np.random.permutation(nsamples)\n",
    "    for i in range(0, nsamples, batch_size):\n",
    "        batch_idx = perm[i:i+batch_size]\n",
    "        if target is not None:\n",
    "            yield data[batch_idx,:], target[batch_idx]\n",
    "        else:\n",
    "            yield data[batch_idx], None\n",
    "\n",
    "def training(model, train_data, train_target, epoch, args):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    optimizer = AdamW(model.parameters(), lr=args['learning_rate'], correct_bias=False)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args['num_warmup_steps'],\n",
    "                                                num_training_steps=args['num_training_steps'])\n",
    "    b = 0\n",
    "    ncorrect = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    batch_size = args['batch_size']\n",
    "    model.train()\n",
    "    for X, y in batch_generator(train_data, train_target, batch_size):\n",
    "        \n",
    "        X_i, X_s, X_p, y = utils.ToTensor(X,y)\n",
    "        \n",
    "        out = model(input_ids=X_i, token_type_ids=X_s, attention_mask=X_p, labels=y)[1]\n",
    "        \n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        total_loss += loss\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])  # Gradient clipping \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = F.softmax(out, dim=1)\n",
    "\n",
    "        ncorrect += (torch.max(out, 1)[1] == y).sum().item()\n",
    "        print(\"Iteration\", b, \"of epoch\", epoch, \"complete\", \"Loss :\", loss.item())\n",
    "        b += 1\n",
    "    total_loss /= len(train_data)\n",
    "    acc = ncorrect/len(train_data) * 100\n",
    "    return acc, loss\n",
    "\n",
    "Iteration 100 of epoch 1 complete\n",
    "\n",
    "def validation(model, eval_data, eval_target, args):\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    b = 0\n",
    "    ncorrect = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    batch_size = args['batch_size']\n",
    "    model.eval()\n",
    "    for X, y in batch_generator(eval_data, eval_target, batch_size):\n",
    "        \n",
    "        X_i, X_s, X_p, y = utils.ToTensor(X,y)\n",
    "        \n",
    "        out = model(input_ids=X_i, token_type_ids=X_s, attention_mask=X_p, labels=y)[1]\n",
    "        \n",
    "        loss = criterion(out, y)   \n",
    "        total_loss += loss\n",
    "        out = F.softmax(out, dim=1)\n",
    "        ncorrect += (torch.max(out, 1)[1] == y).sum().item()\n",
    "        print(\"Batch\", b, \"-> Validation loss:\", loss.item())\n",
    "        b += 1\n",
    "\n",
    "    total_loss /= len(eval_data)\n",
    "    acc = ncorrect/len(eval_data) * 100\n",
    "    return acc, loss\n",
    "\n",
    "def build(learn_data, model_class, pretrained_model, args):\n",
    "    model = model_class.from_pretrained(pretrained_model, num_labels=2)\n",
    "    print(\"Model loaded\")\n",
    "\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(learn_data.iloc[:,:-1], learn_data.iloc[:,-1],                                                            test_size=0.2, random_state=47)\n",
    "\n",
    "    train_acc = [None]*epochs\n",
    "    train_loss = [None]*epochs\n",
    "    val_acc = [None]*epochs\n",
    "    val_los = [None]*epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t_acc, t_loss = training(model, X_train, y_train, epoch, args)\n",
    "             \n",
    "        train_acc[epoch] = t_acc\n",
    "        train_loss[epoch] = t_loss\n",
    "        \n",
    "        v_acc, v_loss = validation(model, X_val, y_val, epoch, args)\n",
    "        val_acc[epoch] = v_acc\n",
    "        val_loss[epoch] = v_loss\n",
    "\n",
    "    model.save_pretrained('trained/models/' + pretrained_model)  \n",
    "    tokenizer.save_pretrained('trained/tokenizers/' + pretrained_model)  \n",
    "\n",
    "    return train_acc, val_acc\n",
    "\n",
    "def test(model, test_data, args):\n",
    "    ncorrect = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    batch_size = args['batch_size']\n",
    "\n",
    "    for X, _ in batch_generator(test_data, batch_size):\n",
    "        #model.eval()\n",
    "        X_i, X_s, X_p = utils.ToTensor(X,y=None)\n",
    "        \n",
    "        out = model(input_ids=X_i, token_type_ids=X_s, attention_mask=X_p)[0]\n",
    "        out = F.softmax(out, dim=1)\n",
    "\n",
    "        ncorrect += (torch.max(out, 1)[1] == y).sum().item()\n",
    "\n",
    "    acc = ncorrect/len(test_data) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model loaded\n/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n\tadd_(Number alpha, Tensor other)\nConsider using one of the following signatures instead:\n\tadd_(Tensor other, *, Number alpha)\nBatch 0 -> Training loss: 0.809839129447937\nBatch 1 -> Training loss: 0.7330045700073242\nBatch 2 -> Training loss: 0.7792940139770508\nBatch 3 -> Training loss: 0.4851895868778229\nBatch 4 -> Training loss: 0.7183745503425598\nBatch 5 -> Training loss: 0.9667508006095886\nBatch 6 -> Training loss: 0.8012155890464783\nBatch 7 -> Training loss: 0.6448269486427307\nBatch 8 -> Training loss: 0.867662787437439\nBatch 9 -> Training loss: 0.7107551097869873\nBatch 10 -> Training loss: 0.3502032160758972\nBatch 11 -> Training loss: 0.49457213282585144\nBatch 12 -> Training loss: 0.491780549287796\nBatch 13 -> Training loss: 0.819337010383606\nBatch 14 -> Training loss: 0.6844422817230225\nBatch 15 -> Training loss: 0.7478179335594177\nBatch 16 -> Training loss: 0.7268087863922119\nBatch 17 -> Training loss: 0.5111587047576904\nBatch 18 -> Training loss: 0.8632374405860901\nBatch 19 -> Training loss: 0.7431402802467346\nBatch 20 -> Training loss: 0.59788978099823\nBatch 21 -> Training loss: 0.6463488340377808\nBatch 22 -> Training loss: 0.5559557676315308\nBatch 23 -> Training loss: 0.5945669412612915\nBatch 24 -> Training loss: 0.6966696977615356\nBatch 25 -> Training loss: 0.6243919134140015\nBatch 26 -> Training loss: 0.6254198551177979\nBatch 27 -> Training loss: 0.5590132474899292\nBatch 28 -> Training loss: 0.5082814693450928\nBatch 29 -> Training loss: 0.5882658362388611\nBatch 30 -> Training loss: 0.9563832879066467\n"
    }
   ],
   "source": [
    "train_acc, val_acc = build(df, model_class, pretrained_model, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_class.from_pretrained(pretrained_model, num_labels=2)\n",
    "X = np.array(df.iloc[:,:-1])\n",
    "y = np.array(df.iloc[:,-1])\n",
    "X_i, X_s, X_p, y = utils.ToTensor(X,y)\n",
    "\n",
    "# Predict\n",
    "Predictions = model(input_ids=X_i[:5],token_type_ids=X_s[:5],attention_mask=X_p[:5])\n",
    "# Training\n",
    "#Predictions = model(input_ids=X_i[:5],token_type_ids=X_s[:5],attention_mask=X_p[:5], labels=y[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit3f9703cefae54e4a8e8d03694f9a0867"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}