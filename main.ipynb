{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "import input_net\n",
    "import utils\n",
    "\n",
    "# Packages\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer\n",
    "from transformers import BartConfig, BartForSequenceClassification, BartTokenizer\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer\n",
    "from transformers import CamembertConfig, CamembertForSequenceClassification, CamembertTokenizer\n",
    "from transformers import DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from transformers import FlaubertConfig, FlaubertForSequenceClassification, FlaubertTokenizer\n",
    "from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer\n",
    "from transformers import XLMConfig, XLMForSequenceClassification, XLMTokenizer\n",
    "from transformers import XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "from transformers import XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Optimizer -> busca info de com s'utilitza\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404290 404290 404290\n"
     ]
    }
   ],
   "source": [
    "TRAIN = \"data/train.csv\"\n",
    "TEST = \"data/test.csv\"\n",
    "INPUT_NET = 'data/input.csv'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "max_len = 70\n",
    "\n",
    "if not path.exists(INPUT_NET):\n",
    "    df = input_net.create_input(TRAIN, INPUT_NET, tokenizer, max_len)\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_NET)\n",
    "\n",
    "tokens_tensor, segments_tensor, attention_tensor, targets_tensor = utils.ToTensor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'albert': (AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer),\n",
    "    'bart': (BartConfig, BartForSequenceClassification, BartTokenizer),\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    'camembert': (CamembertConfig, CamembertForSequenceClassification, CamembertTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer),\n",
    "    'flaubert': (FlaubertConfig, FlaubertForSequenceClassification, FlaubertTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    'xlm roberta': (XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer),\n",
    "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer)\n",
    "}\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]\n",
    "\n",
    "# Mirar aixo:: AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model_type': 'xlm',\n",
    "    'model_name': 'bert-base-cased',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'max_seq_length': 70,\n",
    "    'batch_size': 16, \n",
    "\n",
    "    'num_train_epochs': 4,\n",
    "    'weight_decay': 0,\n",
    "    'learning_rate': 4e-5,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_steps': 0,\n",
    "    'max_grad_norm': 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tokenizer = tokenizer_class.from_pretrained(args['model_name'])\n",
    "\n",
    "model = model_class.from_pretrained(args['model_name'])\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"I\"+str(i) for i in range(70)] + [\"S\"+str(i) for i in range(70)] + [\"P\"+str(i) for i in range(70)] + [\"is_duplicate\"]\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I0</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>...</th>\n",
       "      <th>P61</th>\n",
       "      <th>P62</th>\n",
       "      <th>P63</th>\n",
       "      <th>P64</th>\n",
       "      <th>P65</th>\n",
       "      <th>P66</th>\n",
       "      <th>P67</th>\n",
       "      <th>P68</th>\n",
       "      <th>P69</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>2054</td>\n",
       "      <td>2003</td>\n",
       "      <td>1996</td>\n",
       "      <td>3357</td>\n",
       "      <td>2011</td>\n",
       "      <td>3357</td>\n",
       "      <td>5009</td>\n",
       "      <td>2000</td>\n",
       "      <td>15697</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>2054</td>\n",
       "      <td>2003</td>\n",
       "      <td>1996</td>\n",
       "      <td>2466</td>\n",
       "      <td>1997</td>\n",
       "      <td>12849</td>\n",
       "      <td>10606</td>\n",
       "      <td>16506</td>\n",
       "      <td>1006</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>2129</td>\n",
       "      <td>2064</td>\n",
       "      <td>1045</td>\n",
       "      <td>3623</td>\n",
       "      <td>1996</td>\n",
       "      <td>3177</td>\n",
       "      <td>1997</td>\n",
       "      <td>2026</td>\n",
       "      <td>4274</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>2339</td>\n",
       "      <td>2572</td>\n",
       "      <td>1045</td>\n",
       "      <td>10597</td>\n",
       "      <td>2200</td>\n",
       "      <td>9479</td>\n",
       "      <td>1029</td>\n",
       "      <td>2129</td>\n",
       "      <td>2064</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>2029</td>\n",
       "      <td>2028</td>\n",
       "      <td>21969</td>\n",
       "      <td>1999</td>\n",
       "      <td>2300</td>\n",
       "      <td>21864</td>\n",
       "      <td>2243</td>\n",
       "      <td>2135</td>\n",
       "      <td>5699</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404285</th>\n",
       "      <td>101</td>\n",
       "      <td>2129</td>\n",
       "      <td>2116</td>\n",
       "      <td>3145</td>\n",
       "      <td>22104</td>\n",
       "      <td>2024</td>\n",
       "      <td>2045</td>\n",
       "      <td>1999</td>\n",
       "      <td>1996</td>\n",
       "      <td>14513</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404286</th>\n",
       "      <td>101</td>\n",
       "      <td>2079</td>\n",
       "      <td>2017</td>\n",
       "      <td>2903</td>\n",
       "      <td>2045</td>\n",
       "      <td>2003</td>\n",
       "      <td>2166</td>\n",
       "      <td>2044</td>\n",
       "      <td>2331</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404287</th>\n",
       "      <td>101</td>\n",
       "      <td>2054</td>\n",
       "      <td>2003</td>\n",
       "      <td>2028</td>\n",
       "      <td>9226</td>\n",
       "      <td>1029</td>\n",
       "      <td>102</td>\n",
       "      <td>2054</td>\n",
       "      <td>1005</td>\n",
       "      <td>1055</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404288</th>\n",
       "      <td>101</td>\n",
       "      <td>2054</td>\n",
       "      <td>2003</td>\n",
       "      <td>1996</td>\n",
       "      <td>22480</td>\n",
       "      <td>3296</td>\n",
       "      <td>3465</td>\n",
       "      <td>1997</td>\n",
       "      <td>2542</td>\n",
       "      <td>2096</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404289</th>\n",
       "      <td>101</td>\n",
       "      <td>2054</td>\n",
       "      <td>2003</td>\n",
       "      <td>2066</td>\n",
       "      <td>2000</td>\n",
       "      <td>2031</td>\n",
       "      <td>3348</td>\n",
       "      <td>2007</td>\n",
       "      <td>5542</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404290 rows × 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         I0    I1    I2     I3     I4    I5     I6     I7     I8     I9  ...  \\\n",
       "0       101  2054  2003   1996   3357  2011   3357   5009   2000  15697  ...   \n",
       "1       101  2054  2003   1996   2466  1997  12849  10606  16506   1006  ...   \n",
       "2       101  2129  2064   1045   3623  1996   3177   1997   2026   4274  ...   \n",
       "3       101  2339  2572   1045  10597  2200   9479   1029   2129   2064  ...   \n",
       "4       101  2029  2028  21969   1999  2300  21864   2243   2135   5699  ...   \n",
       "...     ...   ...   ...    ...    ...   ...    ...    ...    ...    ...  ...   \n",
       "404285  101  2129  2116   3145  22104  2024   2045   1999   1996  14513  ...   \n",
       "404286  101  2079  2017   2903   2045  2003   2166   2044   2331   1029  ...   \n",
       "404287  101  2054  2003   2028   9226  1029    102   2054   1005   1055  ...   \n",
       "404288  101  2054  2003   1996  22480  3296   3465   1997   2542   2096  ...   \n",
       "404289  101  2054  2003   2066   2000  2031   3348   2007   5542   1029  ...   \n",
       "\n",
       "        P61  P62  P63  P64  P65  P66  P67  P68  P69  is_duplicate  \n",
       "0         0    0    0    0    0    0    0    0    0             0  \n",
       "1         0    0    0    0    0    0    0    0    0             0  \n",
       "2         0    0    0    0    0    0    0    0    0             0  \n",
       "3         0    0    0    0    0    0    0    0    0             0  \n",
       "4         0    0    0    0    0    0    0    0    0             0  \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...           ...  \n",
       "404285    0    0    0    0    0    0    0    0    0             0  \n",
       "404286    0    0    0    0    0    0    0    0    0             1  \n",
       "404287    0    0    0    0    0    0    0    0    0             0  \n",
       "404288    0    0    0    0    0    0    0    0    0             0  \n",
       "404289    0    0    0    0    0    0    0    0    0             0  \n",
       "\n",
       "[404290 rows x 211 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "def batch_generator(data, target, batch_size):\n",
    "    nsamples = len(data)\n",
    "    perm = np.random.permutation(nsamples)\n",
    "\n",
    "    for i in range(0, nsamples, batch_size):\n",
    "        batch_idx = perm[i:i+batch_size]\n",
    "        if target is not None:\n",
    "            yield data[batch_idx], target[batch_idx]\n",
    "        #else:\n",
    "        #    yield data[batch_idx], None\n",
    "\n",
    "def train(model, X, y, batch_size, optimizer):\n",
    "    model.train()\n",
    "    for batch in batch_generator(train_data, train_target, batch_size):\n",
    "        X = torch.tensor(X, dtype=torch.long, device=device) #al codi original X era un numpy.darray, aqui un pd.df\n",
    "        y = torch.tensor(y, dtype=torch.long, device=device)\n",
    "        \n",
    "        model.zero_grad() #pred = model(input_ids=tokens_tensor[i_min:i_max],token_type_ids=segments_tensor[i_min:i_max],attention_mask=attention_tensor[i_min:i_max], labels=)\n",
    "        out = model(X)\n",
    "        loss = criterion(out, y) #no se si aixo ha de ser diferent\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def validation(model, X, y, batch_size):\n",
    "    model.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(model, learn_data, optimizer, batch_size, epochs):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(learn_data.iloc[:,:-1], learn_data[:,-1], test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_acc = [None]*epochs\n",
    "    train_loss = [None]*epochs\n",
    "    val_acc = [None]*epochs\n",
    "    val_los = [None]*epochs\n",
    "    for epoch in range(epochs):\n",
    "        acc, loss = train(model, X_train, y_train, batch_size, optimizer)\n",
    "        train_acc[epoch] = acc\n",
    "        train_loss[epoch] = loss\n",
    "        \n",
    "        acc, loss = val(model, val_data)\n",
    "        val_acc[epoch] = acc\n",
    "        val_loss[epoch] = loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7fdb4f7f3583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#optimizer, scheduler = self.get_optimizers(num_training_steps=t_total) #comentat pq no se on va\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "#optimizer, scheduler = self.get_optimizers(num_training_steps=t_total) #comentat pq no se on va\n",
    "\n",
    "#hyp to optimise\n",
    "batch_size = 16#,32\n",
    "epochs = 4\n",
    "\n",
    "build(model, df, optimizer, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = model(input_ids=tokens_tensor[:5],token_type_ids=segments_tensor[:5],attention_mask=attention_tensor[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2488, 0.7512],\n",
       "        [0.2490, 0.7510],\n",
       "        [0.2397, 0.7603],\n",
       "        [0.2346, 0.7654],\n",
       "        [0.2338, 0.7662]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(model(input_ids=tokens_tensor[:5],token_type_ids=segments_tensor[:5],attention_mask=attention_tensor[:5])[0], dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
